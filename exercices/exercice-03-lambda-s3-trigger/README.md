# Exercice 03 : Lambda + S3 Trigger - Traitement Automatique de Fichiers

## üéØ Objectifs p√©dagogiques

√Ä la fin de cet exercice, vous serez capable de :
- Cr√©er une fonction AWS Lambda
- Configurer un trigger S3 pour d√©clencher Lambda automatiquement
- G√©rer les permissions IAM entre services
- Impl√©menter diff√©rents sc√©narios de traitement de fichiers

## üìö Pr√©requis

- Compte AWS (Free Tier)
- Exercice 02 compl√©t√© (bucket S3)
- Connaissances de base en Python ou Node.js

## üèóÔ∏è Architecture

```mermaid
sequenceDiagram
    participant User as üë§ Utilisateur
    participant S3 as ü™£ S3 Bucket
    participant Event as üì® S3 Event
    participant Lambda as ‚ö° Lambda
    participant Output as üì¶ R√©sultat
    
    User->>S3: Upload fichier
    S3->>Event: G√©n√®re √©v√©nement
    Event->>Lambda: D√©clenche fonction
    Lambda->>Lambda: Traitement
    Lambda->>Output: R√©sultat (S3, DynamoDB, SNS...)
    Lambda-->>User: Notification (optionnel)
```

---

## üéÆ Sc√©narios Propos√©s

Choisissez le sc√©nario qui vous int√©resse le plus !

### üñºÔ∏è Sc√©nario A : G√©n√©rateur de Miniatures d'Images

**Difficult√© :** ‚≠ê‚≠ê Interm√©diaire

Lorsqu'une image est upload√©e, Lambda g√©n√®re automatiquement une miniature.

```mermaid
graph LR
    A[üì§ Upload image.jpg] --> B[ü™£ images-originales/]
    B --> C[‚ö° Lambda]
    C --> D[ü™£ images-thumbnails/]
    D --> E[üì∑ image_thumb.jpg]
    
    style C fill:#9333ea,stroke:#7c3aed,color:#fff
```

---

### üìä Sc√©nario B : Analyseur de Fichiers CSV

**Difficult√© :** ‚≠ê D√©butant

Lorsqu'un fichier CSV est upload√©, Lambda l'analyse et stocke les statistiques.

```mermaid
graph LR
    A[üì§ Upload data.csv] --> B[ü™£ data-input/]
    B --> C[‚ö° Lambda]
    C --> D[üìä Analyse]
    D --> E[ü™£ data-output/]
    E --> F[üìÑ stats.json]
    
    style C fill:#9333ea,stroke:#7c3aed,color:#fff
```

---

### üìù Sc√©nario C : Syst√®me de Log et Notification

**Difficult√© :** ‚≠ê D√©butant

Chaque upload g√©n√®re un log et envoie une notification par email.

```mermaid
graph LR
    A[üì§ Upload fichier] --> B[ü™£ uploads/]
    B --> C[‚ö° Lambda]
    C --> D[üìù CloudWatch Logs]
    C --> E[üìß SNS Email]
    
    style C fill:#9333ea,stroke:#7c3aed,color:#fff
```

---

### üîê Sc√©nario D : Validation et Quarantaine de Fichiers

**Difficult√© :** ‚≠ê‚≠ê‚≠ê Avanc√©

Les fichiers upload√©s sont analys√©s et d√©plac√©s selon leur validit√©.

```mermaid
graph TD
    A[üì§ Upload fichier] --> B[ü™£ inbox/]
    B --> C[‚ö° Lambda]
    C --> D{Validation}
    D -->|‚úÖ Valide| E[ü™£ approved/]
    D -->|‚ùå Invalide| F[ü™£ quarantine/]
    D -->|üìß| G[Notification]
    
    style C fill:#9333ea,stroke:#7c3aed,color:#fff
```

---

## üìñ Guide D√©taill√© - Sc√©nario B : Analyseur CSV

Ce sc√©nario est recommand√© pour les d√©butants.

### √âtape 1 : Cr√©er le Bucket S3

1. Acc√©dez √† **S3** dans la console AWS
2. Cr√©ez un bucket : `votre-nom-csv-analyzer`
3. Cr√©ez deux dossiers dans le bucket :
   - `input/` - pour les fichiers CSV √† analyser
   - `output/` - pour les r√©sultats

### √âtape 2 : Cr√©er le R√¥le IAM pour Lambda

1. Acc√©dez √† **IAM** ‚Üí **Roles** ‚Üí **Create role**
2. S√©lectionnez **AWS Service** ‚Üí **Lambda**
3. Attachez les policies :
   - `AWSLambdaBasicExecutionRole` (logs CloudWatch)
   - Cr√©ez une policy personnalis√©e pour S3 :

```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "s3:GetObject",
                "s3:PutObject"
            ],
            "Resource": [
                "arn:aws:s3:::votre-nom-csv-analyzer/*"
            ]
        }
    ]
}
```

4. Nommez le r√¥le : `lambda-csv-analyzer-role`

### √âtape 3 : Cr√©er la Fonction Lambda

1. Acc√©dez √† **Lambda** ‚Üí **Create function**
2. Configurez :

| Param√®tre | Valeur |
|-----------|--------|
| Function name | `csv-analyzer` |
| Runtime | Python 3.12 |
| Architecture | x86_64 |
| Execution role | lambda-csv-analyzer-role |

3. Collez le code suivant :

```python
import json
import boto3
import csv
from io import StringIO
from datetime import datetime

s3_client = boto3.client('s3')

def lambda_handler(event, context):
    """
    Analyse un fichier CSV upload√© sur S3 et g√©n√®re des statistiques.
    """
    # R√©cup√©rer les informations de l'√©v√©nement S3
    bucket = event['Records'][0]['s3']['bucket']['name']
    key = event['Records'][0]['s3']['object']['key']
    
    print(f"üìÅ Traitement du fichier: s3://{bucket}/{key}")
    
    try:
        # T√©l√©charger le fichier CSV
        response = s3_client.get_object(Bucket=bucket, Key=key)
        content = response['Body'].read().decode('utf-8')
        
        # Analyser le CSV
        csv_reader = csv.DictReader(StringIO(content))
        rows = list(csv_reader)
        
        # Calculer les statistiques
        stats = {
            'file_name': key,
            'analyzed_at': datetime.now().isoformat(),
            'total_rows': len(rows),
            'columns': list(rows[0].keys()) if rows else [],
            'column_count': len(rows[0].keys()) if rows else 0,
            'sample_data': rows[:3] if len(rows) >= 3 else rows
        }
        
        # G√©n√©rer le nom du fichier de sortie
        output_key = key.replace('input/', 'output/').replace('.csv', '_stats.json')
        
        # Sauvegarder les statistiques
        s3_client.put_object(
            Bucket=bucket,
            Key=output_key,
            Body=json.dumps(stats, indent=2, ensure_ascii=False),
            ContentType='application/json'
        )
        
        print(f"‚úÖ Statistiques sauvegard√©es: s3://{bucket}/{output_key}")
        
        return {
            'statusCode': 200,
            'body': json.dumps({
                'message': 'CSV analys√© avec succ√®s',
                'stats': stats
            })
        }
        
    except Exception as e:
        print(f"‚ùå Erreur: {str(e)}")
        raise e
```

4. Configurez le **Timeout** √† 30 secondes (Configuration ‚Üí General configuration)

### √âtape 4 : Ajouter le Trigger S3

1. Dans la fonction Lambda, cliquez sur **Add trigger**
2. S√©lectionnez **S3**
3. Configurez :

| Param√®tre | Valeur |
|-----------|--------|
| Bucket | votre-nom-csv-analyzer |
| Event types | PUT, POST |
| Prefix | input/ |
| Suffix | .csv |

4. Cochez "I acknowledge..." et cliquez **Add**

```mermaid
flowchart LR
    A[S3 Trigger Configur√©] --> B{√âv√©nement}
    B -->|PUT input/*.csv| C[‚úÖ Lambda d√©clench√©e]
    B -->|PUT output/*.json| D[‚ùå Ignor√©]
    B -->|PUT images/*.png| E[‚ùå Ignor√©]
    
    style C fill:#22c55e,stroke:#166534,color:#fff
    style D fill:#9ca3af,stroke:#6b7280,color:#fff
    style E fill:#9ca3af,stroke:#6b7280,color:#fff
```

### √âtape 5 : Tester

1. Cr√©ez un fichier CSV de test `exemple.csv` :

```csv
nom,age,ville,profession
Alice,28,Paris,D√©veloppeuse
Bob,35,Lyon,Designer
Charlie,42,Marseille,Chef de projet
Diana,31,Toulouse,Data Scientist
Eve,26,Bordeaux,DevOps
```

2. Uploadez dans `s3://votre-nom-csv-analyzer/input/exemple.csv`
3. V√©rifiez CloudWatch Logs pour les logs de la fonction
4. V√©rifiez `s3://votre-nom-csv-analyzer/output/exemple_stats.json`

R√©sultat attendu :
```json
{
  "file_name": "input/exemple.csv",
  "analyzed_at": "2024-12-16T10:30:00.000000",
  "total_rows": 5,
  "columns": ["nom", "age", "ville", "profession"],
  "column_count": 4,
  "sample_data": [
    {"nom": "Alice", "age": "28", "ville": "Paris", "profession": "D√©veloppeuse"},
    {"nom": "Bob", "age": "35", "ville": "Lyon", "profession": "Designer"},
    {"nom": "Charlie", "age": "42", "ville": "Marseille", "profession": "Chef de projet"}
  ]
}
```

---

## üîß D√©pannage

### Erreur : Access Denied

```mermaid
graph TD
    A[Access Denied] --> B{V√©rifier}
    B --> C[Policy IAM du r√¥le Lambda]
    B --> D[Bucket Policy S3]
    B --> E[Permissions du trigger]
```

**Solution :** V√©rifiez que le r√¥le Lambda a les permissions `s3:GetObject` et `s3:PutObject` sur le bucket.

### Erreur : Function Timeout

**Solution :** Augmentez le timeout dans Configuration ‚Üí General configuration (max 15 minutes).

### Lambda non d√©clench√©e

**V√©rifications :**
1. Le fichier est bien dans le pr√©fixe `input/`
2. Le fichier a l'extension `.csv`
3. Le trigger S3 est correctement configur√©

---

## ‚úÖ Validation

### Checklist

- [ ] Bucket S3 cr√©√© avec dossiers input/ et output/
- [ ] R√¥le IAM cr√©√© avec permissions S3 et CloudWatch
- [ ] Fonction Lambda cr√©√©e et configur√©e
- [ ] Trigger S3 ajout√© avec bon pr√©fixe et suffixe
- [ ] Test r√©ussi avec fichier CSV
- [ ] R√©sultat JSON g√©n√©r√© dans output/

---

## üöÄ Challenges Bonus

### Challenge 1 : Ajouter des statistiques num√©riques

Modifiez le code pour calculer la moyenne, min, max des colonnes num√©riques.

### Challenge 2 : Notification par email

Ajoutez SNS pour envoyer un email quand l'analyse est termin√©e.

### Challenge 3 : D√©tection d'anomalies

Ajoutez une logique pour d√©tecter les lignes avec des donn√©es manquantes ou invalides.

---

## üßπ Nettoyage

1. Supprimez le trigger S3 dans Lambda
2. Supprimez la fonction Lambda
3. Supprimez le r√¥le IAM
4. Videz et supprimez le bucket S3

---

## üìö Pour aller plus loin

- [Documentation AWS Lambda](https://docs.aws.amazon.com/lambda/)
- [S3 Event Notifications](https://docs.aws.amazon.com/AmazonS3/latest/userguide/NotificationHowTo.html)
- [Lambda Layers](https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html) pour ajouter des biblioth√®ques
